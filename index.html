<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

  <head>
    <meta name=viewport content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme stolen from Sergey Karayev */
      
      a {
	  color: #1772d0;
	  text-decoration: none;
      }
      
      a:focus,
      a:hover {
	  color: #f09228;
	  text-decoration: none;
      }
      
      body,
      td,
      th,
      tr,
      p,
      a {
	  font-family: 'Lato', Verdana, Helvetica, sans-serif;
	  font-size: 14px
      }
      
      strong {
	  font-family: 'Lato', Verdana, Helvetica, sans-serif;
	  font-size: 14px;
      }
      
      heading {
	  font-family: 'Lato', Verdana, Helvetica, sans-serif;
	  font-size: 22px;
      }
      
      papertitle {
	  font-family: 'Lato', Verdana, Helvetica, sans-serif;
	  font-size: 14px;
	  font-weight: 700
      }
      
      name {
	  font-family: 'Lato', Verdana, Helvetica, sans-serif;
	  font-size: 32px;
      }
      
      .one {
	  width: 160px;
	  height: 160px;
	  position: relative;
      }
      
      .two {
	  width: 160px;
	  height: 160px;
	  position: absolute;
	  transition: opacity .2s ease-in-out;
	  -moz-transition: opacity .2s ease-in-out;
	  -webkit-transition: opacity .2s ease-in-out;
      }
      
      .fade {
	  transition: opacity .2s ease-in-out;
	  -moz-transition: opacity .2s ease-in-out;
	  -webkit-transition: opacity .2s ease-in-out;
      }
      
      span.highlight {
	  background-color: #ffffd0;
      }
    </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Daniel Gongora</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>

  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
	<td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
		<p align="center">
                  <name>Daniel Gongora</name>
		</p>
		<p>I am a Ph.D. candidate at <a href="http://www.tohoku.ac.jp/en/">Tohoku University</a>, where I work at the <a href="https://www.rm.is.tohoku.ac.jp/englishtop/">Human-Robot Informatics laboratory</a>.
		</p>
		<p>
                  I did my Masters at <a href="http://www.tohoku.ac.jp/en/">Tohoku University</a>, where I was advised by Masashi Konyo and funded by <a href="http://www.mext.go.jp">MEXT</a>. I did my bachelors at <a href="www.umsa.bo">Universidad Mayor de San Andr&eacute;s</a>, where I was advised by Mauricio Am&eacute;stegui.
		</p>
		<p align=center>
                  <a href="mailto:daniel@rm.is.tohoku.ac.jp">Email</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=gzSsRZYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                  <a href="http://www.linkedin.com/in/dmgongora/"> LinkedIn </a>
		</p>
              </td>
              <td width="33%">
		<img src="me_circle.jpg" width="250">
              </td>
            </tr>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
		<heading>Research</heading>
		<p>
                  I'm interested in haptics, virtual reality, and human-robot interaction. 
		  In my research, I've worked primarily with vibrotactile feedback but I've also had a chance to experiment with peltier modules for <a href="#thermal">thermal feedback</a>, an ultrasonic array for <a href="#mid_air">mid-air haptic feedback</a>, and with a <a href="#var_friction">variable friction display</a>.
		</p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr>
              <td width="25%"><img src="euro2018.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                  <a href="https://rdcu.be/7Hxp">
                    <papertitle>Vibrotactile Feedback Improves Collision Detection in Fast Playback of First-Person View Videos</papertitle>
                  </a>
                  <br>
                  <strong>Daniel Gongora</strong>, Hikaru Nagano, Masashi Konyo, Satoshi Tadokoro
                  <br>
                  <em>International Conference on Human Haptic Sensing and Touch Enabled Computer Applications (Eurohaptics)</em>, 2018
		  <br>
		  <a href="gongora2018vibrotactile.bib">bibtex</a>

		</p>
		<p>
		  Fast playback of First-Person View (FPV) videos reduces watching time but it also increases the perceived intensity of camera trembling and makes transient events, such as collisions, less evident. Here we propose using camera vibrations as vibrotactile feedback to support collision detection in fast video playback. 
		</p>
		</p>
              </td>
            </tr>

            <tr>
              <td width="25%"><img src="icra2017.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                  <a href="https://ieeexplore.ieee.org/document/7989059/">
                    <papertitle>Collision representation using vibrotactile cues to bimanual impact localization for mobile robot operations</papertitle>
                  </a>
                  <br>
                  <strong>Daniel Gongora</strong>, Hikaru Nagano, Yosuke Suzuki, Masashi Konyo, Satoshi Tadokoro
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA)</em>, 2017
		  <br>
		  <a href="gongora2017collision.bib">bibtex</a>

		</p>
		<p>
		  Unnoticed collisions compromise the success of remote exploratory tasks with mobile robots. We propose a vibrotactile stimulation method to represent frontal collisions that is based on the way people perceive impacts on a bar held with both hands.
		</p>
		</p>
              </td>
            </tr>

	    <tr>
              <td width="25%"><img src="wh2017.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                  <a href="https://ieeexplore.ieee.org/document/7989944/">
                    <papertitle>Vibrotactile rendering of camera motion for bimanual experience of first-person view videos</papertitle>
                  </a>
                  <br>
                  <strong>Daniel Gongora</strong>, Hikaru Nagano, Masashi Konyo, Satoshi Tadokoro
                  <br>
                  <em>World Haptics Conference (WHC)</em>, 2017
		  <br>
		  <a href="gongora2017vibrotactile.bib">bibtex</a>

		</p>
		<p>
		  We propose a vibrotactile rendering method for the motion of a camera in first-person view videos that enables people to feel the movement of the camera with both hands. Concretely, we consider an arrangement of two vibrotactile actuators to render panning movements on the horizontal axis as vibrations that move from hand to hand, and to represent sudden vertical displacements of the camera as transient vibrations on both hands.
		</p>
		</p>
              </td>
            </tr>

	    <tr>
              <td width="25%"><img src="uist2017.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                  <a href="https://dl.acm.org/citation.cfm?id=3131814">
                    <div id="thermal"><papertitle>Towards Intermanual Apparent Motion of Thermal Pulses</papertitle></a>
                  </a>
                  <br>
                  <strong>Daniel Gongora</strong>, Roshan Lalintha Peiris, Kouta Minamizawa
                  <br>
                  <em>ACM Symposium on User Interface Software and Technology (UIST)</em>, 2017
		  <br>
		  <a href="gongora2017towards.bib">bibtex</a>

		</p>
		<p>
		  Perceptual illusions enable designers to go beyond hardware limitations to create rich haptic content. Nevertheless, spatio-temporal interactions for thermal displays have not been studied thoroughly. We focus on the apparent motion of hot and cold thermal pulses delivered at the thenar eminence of both hands. 
		</p>
		</p>
              </td>
            </tr>
	    

          </table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
		<heading>Other Projects</heading>
              </td>
            </tr>
          </table>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	    <tr>
              <td width="25%"><img src="sch2018.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                    <div id="mid_air"><papertitle>Weather Tele-report</papertitle></div>
                  <br>
                  Cao Nan, <strong>Daniel Gongora</strong>, Horie Arata
                  <br>
                  <em>Student Challenge - Eurohaptics</em>, 2018
		  <br>
		  <a href="https://youtu.be/MmQQdLKHn-w">Video</a> 

		</p>
		<p>Weather Telereport is a novel and informative way to check the weather. Weather Telereport  consists of a Weather Box where users insert one hand to experience the weather conditions in a place of interest as if their hand had traveled there. Inside the Weather Box lies an ultrasonic haptic display that represents a wide variety of weather conditions on the hands of the users and it also enables users to control the Weather Box with natural gestures. In addition, the Weather Box is equipped with fans and halogen lamps to further increase the immersiveness. Weather Telereport lends itself for quickly checking the weather before leaving home and for exploring the weather around the world at leisure.</p>
		</p>
              </td>
            </tr>
	    
            <tr>
              <td width="25%"><img src="sch2016.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                    <papertitle>Tele Teku Teku</papertitle>
                  <br>
                  <strong>Daniel Gongora</strong>, Hideto Takenouchi, Gu Wenchao, Yoshihiro Kato
                  <br>
                  <em>Student Challenge - IEEE Haptics Symposium</em>, 2016
		  <font color="red">
		    <strong>
		      (People's choice, <a href="http://2016.hapticssymposium.org/awardwinners">announcement</a>)
		    </strong>
		  </font>
		  <br>
		  <a href="https://youtu.be/q2U_IZ0wkWk">Video</a> 

		</p>
		<p>Tele Teku-Teku is a shared walking experience. It is a system designed for friends wanting to go for a stroll together but unable to do so because one of them is constrained to a certain place. We use the Vybe haptic gaming pad to provide vibrotactile feedback in sync with the footsteps of a distant friend. One of the users wears vibration sensors and carries an avatar robot equipped with an IMU and a camera, the other wears a Head-Mounted Display and sits on a chair enhanced by the gaming pad. Together these technologies set the stage for a rich and engaging experience. Shall we walk?</p>
		</p>
              </td>
            </tr>

	    <tr>
              <td width="25%"><img src="sch2015.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                  <div id="var_friction"><papertitle>Hello HapticWorld</papertitle></div>
                  <br>
                  Dennis Babu, <strong>Daniel Gongora</strong>, Seonghwan Kim, Shunya Sakata
                  <br>
                  <em>Student Challenge - WorldHaptics Conference</em>, 2015
		  <font color="red">
		    <strong>
		      (2nd Place, <a href="http://tpadtablet.org/world-haptics-conference-and-student-innovation-challenge-recap/">announcement</a>)
		    </strong>
		  </font>
		  <br>
		  <a href="https://youtu.be/RPXFvNnCIIY">Video</a> 

		</p>
		<p>Hello HapticWorld uses haptic feedback to encourage kids to explore their surroundings. We use a smartphone equipped with a variable friction display to control a small mobile robot. The variable friction display lets the user feel the inclination of the robot and its distance to obstacles ahead.</p>
		</p>
              </td>
            </tr>

	    	    <tr>
              <td width="25%"><img src="ivrc2015.png" alt="clean-usnob" width="160" height="160"></td>
              <td width="75%" valign="top">
		<p>
		<p>
                    <papertitle>Cockroach-Human Interaction</papertitle>
                  <br>
                  <strong>Daniel Gongora</strong>, Takahito Funamizu
                  <br>
                  <em>International collegiate Virtual Reality Contest (IVRC)</em>, 2015
		  <br>
		  <a href="Somewhere">Details</a> 

		</p>
		<p>Cockroach-Human Interaction lets participants experience the world from the point of view of a cockroach. Users crawl to control a small mobile robot using a custom input interface built around a pair of conveyor belts. Footsteps near the robot trigger vibrations on the user's chest and a lantern pointed towards the robot causes the camera to shake inducing feelings of agitation and anxiety in the user. </p>
		<p>
		  Media (Japanese):
		  <a href="http://akirayou.net/akira/ivrc/2015yosen/">One</a>
		  <a href="http://portal.nifty.com/kiji/150929194679_1.htm">Two</a>
		</p>
		</p>
              </td>
            </tr>


	  </table>

	  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
		<br>
		<p align="right">
                  <font size="2">
                    Design: <a href="https://github.com/jonbarron/jonbarron_website"><strong>Jon Barron</strong></a>.
                  </font>
		</p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>
  </body>

</html>
